{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/sabumjung/Tensorflow/blob/master/01_simple_policy.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "lIobriWaQYQD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#필요한 라이브러리를 임포트하기\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lVgYJOGfQtIe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#bandit arms 생성하기\n",
        "#현재 arm4가 가장 큰 reward를 제공한다.\n",
        "bandit_arms = [0.2,0,-0.2,-2]\n",
        "num_arms = len(bandit_arms)\n",
        "\n",
        "def pullBandit(bandit):\n",
        "    #랜덤 숫자를 얻어오기\n",
        "    result = np.random.randn(1)\n",
        "    if result > bandit:\n",
        "        #결과가 parameter인 bandit값보다 크면 값 1을 리턴함(돈을 얻는 경우)\n",
        "        return 1\n",
        "    else:\n",
        "        #결과가 parameter인 bandit값보다 작으면 값 -1을 리턴함(돈을 잃는 경우)\n",
        "        return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-aTInWfRR5-M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "#네트워크의 피드포워드 부분을 설정한다. \n",
        "#모든 원소의 값이 1인 텐서를 생성한다. => [1,1,1,1]\n",
        "weights = tf.Variable(tf.ones([num_arms]))\n",
        "#activation 함수(소프트맥스)값을 계산한다.\n",
        "output = tf.nn.softmax(weights)\n",
        "\n",
        "#다음 6행은 훈련 과정을 설정한다. \n",
        "#네트워크 상에서 선택한 행동과 보상을 피드한다.\n",
        "#보상과 액션정보를 저장한다.\n",
        "reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
        "action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
        "\n",
        "#loss를 계산하고 이를 이용하여 네트워크를 업데이트한다.\n",
        "responsible_output = tf.slice(output,action_holder,[1])\n",
        "#loss = -log(pi) * A\n",
        "#Action이 어떤 기준선보다 얼마나 더 나은지의 정도를 의미함\n",
        "loss = -(tf.log(responsible_output)*reward_holder)\n",
        "\n",
        "#adamoptimizer를 이용하여 최적화를 수행한다.\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
        "update = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "niLqJmXKUylQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "65057563-df3a-4fac-e25a-b7b8500381d2"
      },
      "cell_type": "code",
      "source": [
        "total_episodes = 1000 #에이전트를 훈련시키기 위한 에피소트의 총 회수를 설정한다.\n",
        "total_reward = np.zeros(num_arms) #bandit arms의 scoreboard를 0으로 설정한다.\n",
        "\n",
        "\n",
        "#변수값을 초기화한다.\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# 텐서플로 그래프를 런칭한다.\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    i = 0\n",
        "    while i < total_episodes:     \n",
        "        #볼츠만 분포에 근거한 action을 선택한다.\n",
        "        actions = sess.run(output)\n",
        "        a = np.random.choice(actions,p=actions)\n",
        "        action = np.argmax(actions == a)\n",
        "\n",
        "        reward = pullBandit(bandit_arms[action]) #bandit arms중 하나를 선택하여 보상을 계산한다..\n",
        "        \n",
        "        #네트워크를 갱신한다.\n",
        "        _,resp,ww = sess.run([update,responsible_output,weights], feed_dict={reward_holder:[reward],action_holder:[action]})\n",
        "        \n",
        "        #스코어의 합을 업데이트 한다.\n",
        "        total_reward[action] += reward\n",
        "        \n",
        "        #매 50회 마다 진행결과를 출력한다.\n",
        "        if i % 50 == 0:\n",
        "            #4개의 bandit arm에 대한 총 보상을 출력한다.\n",
        "            print(\"Running reward for the \" + str(num_arms) + \" arms of the bandit: \" + str(total_reward))\n",
        "        i+=1\n",
        "print(\"\\nThe agent thinks arm \" + str(np.argmax(ww)+1) + \" is the most promising....\")\n",
        "\n",
        "if np.argmax(ww) == np.argmax(-np.array(bandit_arms)):\n",
        "    print(\"...and it was right!\")\n",
        "else:\n",
        "    print(\"...and it was wrong!\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running reward for the 4 arms of the bandit: [-1.  0.  0.  0.]\n",
            "Running reward for the 4 arms of the bandit: [-5. -2.  1. 19.]\n",
            "Running reward for the 4 arms of the bandit: [-9.  0.  3. 25.]\n",
            "Running reward for the 4 arms of the bandit: [-11.   5.  14.  35.]\n",
            "Running reward for the 4 arms of the bandit: [-17.   5.  10.  47.]\n",
            "Running reward for the 4 arms of the bandit: [-17.   3.  14.  69.]\n",
            "Running reward for the 4 arms of the bandit: [-24.   0.  16.  83.]\n",
            "Running reward for the 4 arms of the bandit: [-25.  -2.  20.  98.]\n",
            "Running reward for the 4 arms of the bandit: [-30.  -3.  20. 108.]\n",
            "Running reward for the 4 arms of the bandit: [-30.  -3.  13. 129.]\n",
            "Running reward for the 4 arms of the bandit: [-30.  -5.  11. 143.]\n",
            "Running reward for the 4 arms of the bandit: [-34.  -8.  14. 159.]\n",
            "Running reward for the 4 arms of the bandit: [-34.  -7.  14. 170.]\n",
            "Running reward for the 4 arms of the bandit: [-36. -10.  18. 185.]\n",
            "Running reward for the 4 arms of the bandit: [-41. -10.  18. 198.]\n",
            "Running reward for the 4 arms of the bandit: [-45. -10.  20. 212.]\n",
            "Running reward for the 4 arms of the bandit: [-49.  -9.  23. 236.]\n",
            "Running reward for the 4 arms of the bandit: [-50.  -7.  24. 252.]\n",
            "Running reward for the 4 arms of the bandit: [-47.  -7.  25. 272.]\n",
            "Running reward for the 4 arms of the bandit: [-48.  -5.  23. 289.]\n",
            "\n",
            "The agent thinks arm 4 is the most promising....\n",
            "...and it was right!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NyRM2a6fU3A4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}