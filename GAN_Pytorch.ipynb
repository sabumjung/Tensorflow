{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hello, Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabumjung/Tensorflow/blob/master/GAN_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "n12_Cn0oxCNN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "b20dbdc1-fd7f-429d-d660-78b37f1032e1"
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x582b6000 @  0x7f4eeb6722a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pnsg2COawMBf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2827
        },
        "outputId": "4f7f228d-fe04-479c-cf32-2c8dee35b76d"
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "# Generative Adversarial Networks (GAN) example in PyTorch.\n",
        "# See related blog post at https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f#.sch4xgsa9\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Data params\n",
        "data_mean = 4\n",
        "data_stddev = 1.25\n",
        "\n",
        "# Model params\n",
        "g_input_size = 1     # Random noise dimension coming into generator, per output vector\n",
        "g_hidden_size = 50   # Generator complexity\n",
        "g_output_size = 1    # size of generated output vector\n",
        "d_input_size = 100   # Minibatch size - cardinality of distributions\n",
        "d_hidden_size = 50   # Discriminator complexity\n",
        "d_output_size = 1    # Single dimension for 'real' vs. 'fake'\n",
        "minibatch_size = d_input_size\n",
        "\n",
        "d_learning_rate = 2e-4  # 2e-4\n",
        "g_learning_rate = 2e-4\n",
        "optim_betas = (0.9, 0.999)\n",
        "num_epochs = 30000\n",
        "print_interval = 200\n",
        "d_steps = 1  # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
        "g_steps = 1\n",
        "\n",
        "# ### Uncomment only one of these\n",
        "#(name, preprocess, d_input_func) = (\"Raw data\", lambda data: data, lambda x: x)\n",
        "(name, preprocess, d_input_func) = (\"Data and variances\", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)\n",
        "\n",
        "print(\"Using data [%s]\" % (name))\n",
        "\n",
        "# ##### DATA: Target data and generator input data\n",
        "\n",
        "def get_distribution_sampler(mu, sigma):\n",
        "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n)))  # Gaussian\n",
        "\n",
        "def get_generator_input_sampler():\n",
        "    return lambda m, n: torch.rand(m, n)  # Uniform-dist data into generator, _NOT_ Gaussian\n",
        "\n",
        "# ##### MODELS: Generator model and discriminator model\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.map1 = nn.Linear(input_size, hidden_size)\n",
        "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.map3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.elu(self.map1(x))\n",
        "        x = F.sigmoid(self.map2(x))\n",
        "        return self.map3(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.map1 = nn.Linear(input_size, hidden_size)\n",
        "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.map3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.elu(self.map1(x))\n",
        "        x = F.elu(self.map2(x))\n",
        "        return F.sigmoid(self.map3(x))\n",
        "\n",
        "def extract(v):\n",
        "    return v.data.storage().tolist()\n",
        "\n",
        "def stats(d):\n",
        "    return [np.mean(d), np.std(d)]\n",
        "\n",
        "def decorate_with_diffs(data, exponent):\n",
        "    mean = torch.mean(data.data, 1, keepdim=True)\n",
        "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0])\n",
        "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
        "    return torch.cat([data, diffs], 1)\n",
        "\n",
        "d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
        "gi_sampler = get_generator_input_sampler()\n",
        "G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
        "D = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size)\n",
        "criterion = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
        "d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)\n",
        "g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for d_index in range(d_steps):\n",
        "        # 1. Train D on real+fake\n",
        "        D.zero_grad()\n",
        "\n",
        "        #  1A: Train D on real\n",
        "        d_real_data = Variable(d_sampler(d_input_size))\n",
        "        d_real_decision = D(preprocess(d_real_data))\n",
        "        d_real_error = criterion(d_real_decision, Variable(torch.ones(1)))  # ones = true\n",
        "        d_real_error.backward() # compute/store gradients, but don't change params\n",
        "\n",
        "        #  1B: Train D on fake\n",
        "        d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
        "        d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
        "        d_fake_decision = D(preprocess(d_fake_data.t()))\n",
        "        d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1)))  # zeros = fake\n",
        "        d_fake_error.backward()\n",
        "        d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
        "\n",
        "    for g_index in range(g_steps):\n",
        "        # 2. Train G on D's response (but DO NOT train D on these labels)\n",
        "        G.zero_grad()\n",
        "\n",
        "        gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
        "        g_fake_data = G(gen_input)\n",
        "        dg_fake_decision = D(preprocess(g_fake_data.t()))\n",
        "        g_error = criterion(dg_fake_decision, Variable(torch.ones(1)))  # we want to fool, so pretend it's all genuine\n",
        "\n",
        "        g_error.backward()\n",
        "        g_optimizer.step()  # Only optimizes G's parameters\n",
        "\n",
        "    if epoch % print_interval == 0:\n",
        "        print(\"%s: D: %s/%s G: %s (Real: %s, Fake: %s) \" % (epoch,\n",
        "                                                            extract(d_real_error)[0],\n",
        "                                                            extract(d_fake_error)[0],\n",
        "                                                            extract(g_error)[0],\n",
        "                                                            stats(extract(d_real_data)),\n",
        "                                                            stats(extract(d_fake_data))))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using data [Data and variances]\n",
            "0: D: 0.533979058265686/0.6763006448745728 G: 0.7056097388267517 (Real: [3.9419694566726684, 1.295606297750326], Fake: [0.3706138399243355, 0.0061278857015142196]) \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "200: D: 9.536747711536009e-07/0.29150837659835815 G: 1.410908579826355 (Real: [3.976803275346756, 1.312490710979874], Fake: [-0.5963412189483642, 0.00875228345546433]) \n",
            "400: D: 1.430521751899505e-05/0.4010731279850006 G: 1.0653671026229858 (Real: [3.8692291215062142, 1.260613325076887], Fake: [-0.4006045714020729, 0.096216268624187]) \n",
            "600: D: 8.583437011111528e-05/0.3431597352027893 G: 1.1056472063064575 (Real: [4.135439375638962, 1.2556383432994032], Fake: [-0.32976860910654066, 0.28417734125543703]) \n",
            "800: D: 0.00313899340108037/0.03483635187149048 G: 3.400461196899414 (Real: [3.9534534454345702, 1.0504362761427666], Fake: [0.9826757052540779, 0.33452772037519785]) \n",
            "1000: D: 0.060236480087041855/0.09851933270692825 G: 1.3072092533111572 (Real: [3.930608584284782, 1.2738655539421637], Fake: [2.0683742907643317, 1.1930398833308808]) \n",
            "1200: D: 0.7992941737174988/0.16093915700912476 G: 0.5758013129234314 (Real: [4.223845323324204, 1.3036187762931395], Fake: [4.407263022661209, 1.224787850003911]) \n",
            "1400: D: 1.4697532653808594/0.5853139162063599 G: 0.3152143955230713 (Real: [4.051695240736008, 1.106793384334521], Fake: [5.142223873138428, 1.4226315978298174]) \n",
            "1600: D: 1.1607699394226074/0.6670248508453369 G: 0.854047954082489 (Real: [4.001480894088745, 1.1876598871577342], Fake: [5.3475372445583345, 1.568469797142119]) \n",
            "1800: D: 0.5953769087791443/0.45875683426856995 G: 0.8606585264205933 (Real: [4.259885668754578, 1.10777706653082], Fake: [5.380340480804444, 1.2934117626563617]) \n",
            "2000: D: 0.6893764138221741/0.4879772961139679 G: 0.8882520794868469 (Real: [3.8155258017778397, 1.4863397148956632], Fake: [4.453090550899506, 1.4805999362648006]) \n",
            "2200: D: 0.616755485534668/1.0456968545913696 G: 0.6435115933418274 (Real: [3.926506616473198, 1.1164423408016755], Fake: [3.8938528060913087, 1.1483980112337633]) \n",
            "2400: D: 0.6559412479400635/0.8107366561889648 G: 0.5472886562347412 (Real: [4.087051388025284, 1.1502583413290084], Fake: [3.376029257774353, 1.0966851456218667]) \n",
            "2600: D: 0.867113471031189/0.39691802859306335 G: 0.8227273225784302 (Real: [4.0099792373180385, 1.074064591962088], Fake: [3.181651394367218, 1.2708877009620938]) \n",
            "2800: D: 1.1263395547866821/0.571868360042572 G: 0.5998796224594116 (Real: [3.9153993356227876, 1.1518083642272825], Fake: [4.602799235582352, 1.2149331778317556]) \n",
            "3000: D: 0.6719867587089539/0.8601272702217102 G: 0.6968206763267517 (Real: [4.214624704122543, 1.2360698773807597], Fake: [4.768919060230255, 1.3407800238844667]) \n",
            "3200: D: 0.8575979471206665/0.7723808288574219 G: 0.7069602012634277 (Real: [4.188596481084824, 1.2466107736649925], Fake: [3.958922859430313, 1.3706007358814036]) \n",
            "3400: D: 0.621111273765564/0.8098672032356262 G: 0.5191475749015808 (Real: [3.8535401725769045, 1.20899375250675], Fake: [3.6289806962013245, 1.116735662158623]) \n",
            "3600: D: 0.40286779403686523/0.3462134897708893 G: 0.8465715646743774 (Real: [4.099969886541366, 1.2542229327536236], Fake: [3.1120574843883513, 1.3497133698020014]) \n",
            "3800: D: 0.8086065053939819/0.8404768109321594 G: 0.830445408821106 (Real: [3.945061768293381, 1.1573042465471528], Fake: [4.420131945610047, 1.3762187183182153]) \n",
            "4000: D: 0.7511809468269348/0.47077715396881104 G: 0.6059637665748596 (Real: [4.001014312505722, 1.2097944676696495], Fake: [4.97672835111618, 1.1807056243659344]) \n",
            "4200: D: 0.573287308216095/0.8066962361335754 G: 0.758123517036438 (Real: [4.103970704383682, 1.191788892099224], Fake: [3.8829279673099517, 1.1844033002806753]) \n",
            "4400: D: 0.4274744987487793/0.8170832395553589 G: 0.664097785949707 (Real: [4.010166417360306, 1.2623215900359674], Fake: [3.258335910439491, 1.3202542564083009]) \n",
            "4600: D: 0.6121918559074402/0.4910232126712799 G: 0.8654781579971313 (Real: [3.8746470648050306, 1.3451400039589831], Fake: [4.139976609945297, 0.9715842012412405]) \n",
            "4800: D: 0.5120668411254883/0.789958119392395 G: 0.7535061240196228 (Real: [4.066530567407608, 1.1622502121345015], Fake: [4.606278488636017, 1.059695712781376]) \n",
            "5000: D: 0.7351316809654236/0.767515242099762 G: 0.8229325413703918 (Real: [3.7646299213171006, 1.2190138292857151], Fake: [4.233731279373169, 1.1924706216229335]) \n",
            "5200: D: 0.4127226769924164/0.42851579189300537 G: 0.9042924642562866 (Real: [4.319215865135193, 1.2116967916474455], Fake: [3.5217305755615236, 1.36422751424869]) \n",
            "5400: D: 0.9554659724235535/0.8089336156845093 G: 0.5952167510986328 (Real: [4.203817512989044, 1.1806387092399684], Fake: [3.7419490569829943, 1.4118420749923857]) \n",
            "5600: D: 1.031659483909607/0.4951629936695099 G: 0.5750296711921692 (Real: [4.059656809568406, 1.2055709760890596], Fake: [3.977827218770981, 1.3207611760923441]) \n",
            "5800: D: 0.5879007577896118/0.6083439588546753 G: 0.7959603071212769 (Real: [4.13128834426403, 1.2005676544120678], Fake: [4.015673534870148, 1.1754707092745968]) \n",
            "6000: D: 0.7794389724731445/0.7865293025970459 G: 1.0288554430007935 (Real: [3.807646552324295, 1.183023657750788], Fake: [3.890220898389816, 1.1726294896682516]) \n",
            "6200: D: 0.5790171027183533/0.7450777292251587 G: 0.8316390514373779 (Real: [3.9719936943054197, 1.248875902036664], Fake: [4.015527901649475, 1.3256243604581974]) \n",
            "6400: D: 0.6113033890724182/0.5924723744392395 G: 0.6269444823265076 (Real: [4.153168330192566, 1.1748500536366864], Fake: [4.025294796228409, 1.1520977152488363]) \n",
            "6600: D: 0.8621293902397156/1.0580304861068726 G: 0.7612944841384888 (Real: [3.879104314446449, 1.171145394300581], Fake: [3.839311021566391, 1.2100970576815726]) \n",
            "6800: D: 0.4412343502044678/0.442910373210907 G: 0.7944496273994446 (Real: [4.004003840684891, 1.1427152400740042], Fake: [4.036492985486984, 1.269457970829955]) \n",
            "7000: D: 1.0001426935195923/0.4465261399745941 G: 0.46091893315315247 (Real: [3.9548762699961664, 1.2960838412176314], Fake: [4.236315935850143, 1.0741139355336162]) \n",
            "7200: D: 0.3624270558357239/0.72365403175354 G: 0.9653005003929138 (Real: [3.7755353659763933, 1.46218008640164], Fake: [4.07654436826706, 1.2611710681246846]) \n",
            "7400: D: 1.0471831560134888/0.678316593170166 G: 0.5993598699569702 (Real: [3.7738456764817236, 1.2320829430628117], Fake: [3.7967801129817964, 1.275508792897765]) \n",
            "7600: D: 1.0706861019134521/0.3952096700668335 G: 0.5866841077804565 (Real: [4.023785593211651, 1.3334876632572858], Fake: [3.940349242091179, 1.4828991791727373]) \n",
            "7800: D: 0.3524567186832428/1.1587189435958862 G: 1.3912533521652222 (Real: [3.9186233186721804, 1.2898290233382845], Fake: [3.4925554543733597, 1.4009766476275185]) \n",
            "8000: D: 0.7104921340942383/0.5977170467376709 G: 0.4311211109161377 (Real: [4.180004969835282, 1.3002608066621126], Fake: [4.250117772817612, 1.320954023486491]) \n",
            "8200: D: 0.5540534853935242/0.8526831269264221 G: 0.45326054096221924 (Real: [4.034788793325424, 1.0966828049554584], Fake: [3.714304035305977, 1.2334571031131663]) \n",
            "8400: D: 0.7046470642089844/0.5388768911361694 G: 0.20040090382099152 (Real: [4.045274852514267, 1.1922561303440096], Fake: [4.27544432669878, 1.4716761200931825]) \n",
            "8600: D: 0.8646952509880066/0.6537748575210571 G: 0.7798936367034912 (Real: [4.182520121335983, 1.205315216828364], Fake: [4.161751925945282, 0.9240316280535433]) \n",
            "8800: D: 0.7832276821136475/1.1678742170333862 G: 0.5158333778381348 (Real: [3.8673308908939363, 1.1873087552045778], Fake: [3.5956922894716263, 1.3329743160017924]) \n",
            "9000: D: 0.7312796711921692/0.6002985239028931 G: 1.463887333869934 (Real: [4.063604148626328, 1.3412955013917047], Fake: [4.759631550312042, 1.2128109758719647]) \n",
            "9200: D: 0.7457257509231567/0.6646597981452942 G: 0.7746201753616333 (Real: [3.904740108847618, 1.4072545087298218], Fake: [3.8525737845897674, 1.117284801640004]) \n",
            "9400: D: 0.7470396161079407/0.80413419008255 G: 1.028707504272461 (Real: [4.0913948059082035, 1.1720079675141597], Fake: [4.136471505761147, 0.886393069038761]) \n",
            "9600: D: 1.1846734285354614/0.22495141625404358 G: 1.152194619178772 (Real: [3.978233775496483, 1.1813145739004058], Fake: [3.977639403641224, 1.5223867760434229]) \n",
            "9800: D: 0.8149555325508118/0.6160467863082886 G: 0.921410858631134 (Real: [3.9969738379120825, 1.1865411525741518], Fake: [4.152956963777542, 1.2675847125670279]) \n",
            "10000: D: 0.5956575870513916/0.8235597610473633 G: 0.8404874801635742 (Real: [3.738764567375183, 1.020812313903337], Fake: [4.007844258546829, 1.3943364149993087]) \n",
            "10200: D: 0.5315038561820984/0.8844706416130066 G: 0.8734145164489746 (Real: [4.087299008369445, 1.2689927680217326], Fake: [3.6287750473618505, 1.2223155354704738]) \n",
            "10400: D: 0.8110432624816895/0.3336523771286011 G: 0.7543831467628479 (Real: [4.034972485005856, 1.2617058388010551], Fake: [4.376634278893471, 1.3470587348515703]) \n",
            "10600: D: 0.6094682216644287/1.0314538478851318 G: 0.48281875252723694 (Real: [3.9922527140006423, 1.2546230825989226], Fake: [3.9671727019548415, 1.28782185432774]) \n",
            "10800: D: 0.5744351744651794/0.7970643639564514 G: 0.5561383366584778 (Real: [3.9619731068611146, 1.3066223918256474], Fake: [3.6330837962031364, 1.2024234438531893]) \n",
            "11000: D: 0.1989782303571701/0.5661221146583557 G: 0.7031906247138977 (Real: [3.872840651422739, 1.2185255208822425], Fake: [4.354680628180504, 1.3395439463955283]) \n",
            "11200: D: 0.3529469966888428/0.36568567156791687 G: 0.7730020880699158 (Real: [3.817530750576407, 1.328591508041369], Fake: [4.184566876292228, 1.1418410599423603]) \n",
            "11400: D: 0.8818708658218384/0.7232216596603394 G: 0.8193690776824951 (Real: [4.131772930622101, 1.2168637446845922], Fake: [3.599537790417671, 1.1352077610478761]) \n",
            "11600: D: 0.6400876045227051/0.544452965259552 G: 1.1230934858322144 (Real: [3.819454293139279, 1.350081725551635], Fake: [4.372331206798553, 1.2042431114763172]) \n",
            "11800: D: 0.7144003510475159/0.6163376569747925 G: 0.9951793551445007 (Real: [4.0772461290657525, 1.2501253458891062], Fake: [4.26256305038929, 1.3732352632689302]) \n",
            "12000: D: 0.8859462141990662/0.734876275062561 G: 1.113082766532898 (Real: [3.951043405532837, 1.207933811125597], Fake: [3.8587338572740553, 1.2489976629683888]) \n",
            "12200: D: 0.5851386189460754/0.45219123363494873 G: 0.39811617136001587 (Real: [3.9203888058662413, 1.1683153028314799], Fake: [4.037081163525581, 1.322286513250808]) \n",
            "12400: D: 0.9850795269012451/0.5238222479820251 G: 1.632846474647522 (Real: [3.9330810678005217, 1.1707642895718584], Fake: [4.047552617788315, 1.2749301120078755]) \n",
            "12600: D: 0.4029928147792816/0.39911511540412903 G: 0.9446253180503845 (Real: [3.9455822813510895, 1.2210637407470015], Fake: [3.955087628364563, 1.2373677883456202]) \n",
            "12800: D: 0.5891227126121521/0.5911551117897034 G: 0.862642228603363 (Real: [4.024985044598579, 1.2123947916675362], Fake: [3.9745228523015976, 1.2738607472575512]) \n",
            "13000: D: 0.196206733584404/0.5669671297073364 G: 1.1147733926773071 (Real: [3.911954107284546, 1.3028693472495227], Fake: [4.050638455152511, 1.1676009879937364]) \n",
            "13200: D: 0.5422227382659912/0.24528370797634125 G: 0.7211909890174866 (Real: [4.055342974662781, 1.1228625613674608], Fake: [3.9410500144958496, 1.4972743013068361]) \n",
            "13400: D: 0.43188780546188354/0.5125129222869873 G: 1.5109859704971313 (Real: [4.07533717572689, 1.1501075751114151], Fake: [4.058389244675636, 1.1540447427500418]) \n",
            "13600: D: 1.409837007522583/0.300651878118515 G: 0.7389125823974609 (Real: [4.1271521592140195, 1.1258122417314107], Fake: [3.983773375749588, 1.419444690245484]) \n",
            "13800: D: 0.807732105255127/0.5450977683067322 G: 0.5381813645362854 (Real: [4.284360946416855, 1.2123030956216576], Fake: [4.119150745868683, 1.2126021487957785]) \n",
            "14000: D: 0.15821532905101776/0.6397086381912231 G: 0.6879462599754333 (Real: [4.06978740811348, 1.2960165620621833], Fake: [4.064827753305435, 1.2590589158214576]) \n",
            "14200: D: 0.5330981612205505/0.6254235506057739 G: 1.2440974712371826 (Real: [4.091215859055519, 1.2989048578396698], Fake: [4.006528826355934, 1.2786189985913776]) \n",
            "14400: D: 0.16454477608203888/0.4561045467853546 G: 0.7049417495727539 (Real: [3.9316241157054903, 1.1506321125134353], Fake: [4.137120472192764, 1.1703779129489074]) \n",
            "14600: D: 0.32087525725364685/0.17437498271465302 G: 1.1827447414398193 (Real: [4.025495512485504, 1.1897571325803], Fake: [4.335301415324211, 1.061816929954186]) \n",
            "14800: D: 0.4575570821762085/0.8045894503593445 G: 0.5974690318107605 (Real: [4.196213937997818, 1.0610317501399371], Fake: [4.010902755260467, 1.2529095467101707]) \n",
            "15000: D: 0.30229878425598145/0.9075177311897278 G: 1.4843626022338867 (Real: [3.831365512609482, 1.1332213931997008], Fake: [4.326546245217323, 1.1493412322250636]) \n",
            "15200: D: 0.7783305644989014/0.3334202170372009 G: 1.1728709936141968 (Real: [3.9561772525310515, 1.3114400620231612], Fake: [4.172615849375725, 1.2323240175838122]) \n",
            "15400: D: 0.402529239654541/0.13513566553592682 G: 2.3687469959259033 (Real: [4.012276997566223, 1.3491040772741387], Fake: [3.901472887992859, 1.2746023398956068]) \n",
            "15600: D: 0.11206898093223572/0.5743662714958191 G: 2.25626802444458 (Real: [3.8490454065799713, 1.3800152611701837], Fake: [4.103622637987137, 1.2097268705757669]) \n",
            "15800: D: 1.8955110311508179/0.3605153262615204 G: 2.418231725692749 (Real: [4.010824762582779, 1.1333108262334066], Fake: [4.08179168343544, 1.0964157373404504]) \n",
            "16000: D: 0.8329992890357971/0.15681804716587067 G: 2.520171642303467 (Real: [4.121674568057061, 1.1678951446476247], Fake: [4.02984005689621, 1.1270156212145432]) \n",
            "16200: D: 0.7245352864265442/0.15221838653087616 G: 1.9577170610427856 (Real: [4.034216539561749, 1.3231447242027667], Fake: [3.896836074590683, 1.1606537667484222]) \n",
            "16400: D: 0.9165846109390259/0.1366175413131714 G: 1.7422654628753662 (Real: [3.9454709687829017, 1.5476305102300456], Fake: [4.008566263914108, 1.330125036034528]) \n",
            "16600: D: 0.8851569294929504/0.535897433757782 G: 1.5619515180587769 (Real: [3.7489046734571456, 1.3505972064421443], Fake: [3.9454064166545866, 1.2693497947172534]) \n",
            "16800: D: 0.029469739645719528/0.40018230676651 G: 1.6683917045593262 (Real: [4.069164712429046, 1.3201218871553104], Fake: [4.265564638376236, 1.1425526910883783]) \n",
            "17000: D: 2.122349500656128/0.49449339509010315 G: 1.3230640888214111 (Real: [4.073918783441186, 1.3796279948310888], Fake: [4.095329998731613, 1.2645489947121968]) \n",
            "17200: D: 0.03703733906149864/0.20238728821277618 G: 2.5171420574188232 (Real: [4.039386374354362, 1.4357784392301618], Fake: [4.073105636835098, 1.1950267154952772]) \n",
            "17400: D: 0.15595842897891998/0.16250810027122498 G: 1.0270577669143677 (Real: [3.988040964603424, 1.2412656887673068], Fake: [4.187431304454804, 1.3687000904388045]) \n",
            "17600: D: 0.9890793561935425/0.13673411309719086 G: 1.9064536094665527 (Real: [4.059902173280716, 1.2194720040651255], Fake: [4.020990538597107, 1.2178024426359906]) \n",
            "17800: D: 0.20202860236167908/0.25859391689300537 G: 1.9362163543701172 (Real: [4.086336057186127, 1.265044521995482], Fake: [3.838719037771225, 1.333137966593964]) \n",
            "18000: D: 0.041385192424058914/0.4749124050140381 G: 2.1731796264648438 (Real: [3.881741183400154, 1.2967913612973845], Fake: [3.7460743021965026, 1.2535431326417135]) \n",
            "18200: D: 1.5746994018554688/0.1112731546163559 G: 1.8700107336044312 (Real: [4.16349179148674, 1.2520503945490284], Fake: [4.365291267633438, 1.1504124790217862]) \n",
            "18400: D: 0.08742374926805496/0.7048715949058533 G: 1.7450429201126099 (Real: [4.031842876672744, 1.192898970416546], Fake: [3.872732365131378, 1.1910289047893714]) \n",
            "18600: D: 1.2041527032852173/0.3130490481853485 G: 0.4965238869190216 (Real: [4.144102129936218, 1.1444299547974286], Fake: [3.91361243724823, 1.3789556763829425]) \n",
            "18800: D: 0.6861724257469177/0.4981834292411804 G: 0.60903400182724 (Real: [3.9704037928581237, 1.2920973765130261], Fake: [4.250242639780044, 1.2844381480681109]) \n",
            "19000: D: 2.1797680854797363/0.2560233175754547 G: 2.116917133331299 (Real: [4.16398646235466, 1.1167089668357135], Fake: [4.244500613212585, 1.0909612343761272]) \n",
            "19200: D: 1.240075945854187/2.0864920616149902 G: 1.7914992570877075 (Real: [4.1091152501106265, 1.060309301195108], Fake: [3.9645286691188812, 1.3762504842379002]) \n",
            "19400: D: 0.6617438793182373/0.8212082982063293 G: 1.283188819885254 (Real: [3.933962131142616, 1.174315296352694], Fake: [4.388890017271041, 1.1157857605966583]) \n",
            "19600: D: 0.16154269874095917/0.29092928767204285 G: 2.025941848754883 (Real: [3.955257034301758, 1.162986092286188], Fake: [4.088832787275314, 1.16080377920133]) \n",
            "19800: D: 0.07340043038129807/0.15930919349193573 G: 2.2931342124938965 (Real: [4.031284822225571, 1.1990226447160885], Fake: [4.323463025093079, 1.099630256274596]) \n",
            "20000: D: 0.5556071996688843/0.1616480052471161 G: 1.1670851707458496 (Real: [4.097689204216003, 1.0641072020965674], Fake: [4.106812651157379, 1.338794274583799]) \n",
            "20200: D: 0.005098922643810511/0.09680096805095673 G: 1.5315930843353271 (Real: [3.8871317541599275, 1.36545149933949], Fake: [4.158591033220291, 1.2453040797209152]) \n",
            "20400: D: 0.8261340856552124/0.37919190526008606 G: 1.5891152620315552 (Real: [4.289971259832382, 1.227913587293524], Fake: [3.8000794541835785, 1.260764302411828]) \n",
            "20600: D: 1.2012053728103638/0.8955541849136353 G: 0.5595094561576843 (Real: [4.080187184810638, 1.02898246884929], Fake: [4.049968478679657, 1.3573677302446956]) \n",
            "20800: D: 0.43637534976005554/0.3042019009590149 G: 0.3408524692058563 (Real: [3.867150501012802, 1.1946298911401785], Fake: [4.491595323681832, 1.3045089944754469]) \n",
            "21000: D: 0.9306063652038574/1.171776294708252 G: 0.4618552625179291 (Real: [4.049478039741516, 1.183732194491143], Fake: [4.238153709173202, 1.2920994705066435]) \n",
            "21200: D: 0.17677728831768036/0.21753884851932526 G: 0.976824939250946 (Real: [3.886627904176712, 1.0784429158635507], Fake: [4.216854010820389, 1.0787524928357886]) \n",
            "21400: D: 0.31252235174179077/1.2465267181396484 G: 0.7767186760902405 (Real: [4.160169066190719, 1.2104244222244844], Fake: [4.147707026004792, 1.264539623470824]) \n",
            "21600: D: 0.4389399588108063/0.7625219225883484 G: 2.6029605865478516 (Real: [3.994866293668747, 1.2959555510239915], Fake: [3.93358794093132, 1.31785389957186]) \n",
            "21800: D: 0.2867586314678192/1.5662813186645508 G: 0.4152779281139374 (Real: [4.131416213214397, 1.2837416792543788], Fake: [4.131704256534577, 1.2984808657355944]) \n",
            "22000: D: 0.8155288100242615/0.6113138198852539 G: 0.27146098017692566 (Real: [3.7754180961847306, 1.1471406097650416], Fake: [4.570013206005097, 1.1908221173312747]) \n",
            "22200: D: 0.4412458539009094/0.9032334089279175 G: 0.23613804578781128 (Real: [4.049927339553833, 1.145836326772876], Fake: [3.7522304114699363, 1.3534390058337977]) \n",
            "22400: D: 1.4246020317077637/1.5448201894760132 G: 0.2474571019411087 (Real: [3.822366689443588, 1.2560669115708964], Fake: [3.6443462121486663, 1.2875238017005821]) \n",
            "22600: D: 0.22116728127002716/0.1574513465166092 G: 0.6212381720542908 (Real: [3.8315926045179367, 1.320386280793293], Fake: [4.50172208070755, 1.6302620550386329]) \n",
            "22800: D: 0.20087966322898865/1.4021408557891846 G: 1.1201918125152588 (Real: [4.099720892906189, 1.1871816568438454], Fake: [4.407003029584884, 1.4124983317546784]) \n",
            "23000: D: 0.3930828273296356/0.8249830007553101 G: 0.28916892409324646 (Real: [3.882778364419937, 1.1642224612719916], Fake: [3.488080321550369, 1.0347930062911854]) \n",
            "23200: D: 1.7498985528945923/0.23668119311332703 G: 1.5864737033843994 (Real: [4.122381198257208, 1.3894765859981937], Fake: [4.092170800566674, 1.2143895951969155]) \n",
            "23400: D: 0.9024649262428284/0.18768511712551117 G: 0.7538718581199646 (Real: [4.19037486076355, 1.2263942471735976], Fake: [4.9204838240146636, 1.560647298531273]) \n",
            "23600: D: 1.172851800918579/0.10626926273107529 G: 0.8140656352043152 (Real: [4.2633580383658405, 1.3332767572449322], Fake: [4.75378692984581, 1.2763653060651143]) \n",
            "23800: D: 1.0711454153060913/0.2825368344783783 G: 0.36864641308784485 (Real: [4.055235511660576, 1.212973572816502], Fake: [4.005420026779174, 1.3448924973841314]) \n",
            "24000: D: 1.3087449073791504/0.7924565672874451 G: 0.8930127024650574 (Real: [4.0034674221277236, 1.33030543701839], Fake: [3.332769073843956, 1.1085596596530753]) \n",
            "24200: D: 0.7697595357894897/0.5391002297401428 G: 1.0182210206985474 (Real: [3.97168004989624, 1.3527017259080332], Fake: [4.165760871171951, 1.2190192711633927]) \n",
            "24400: D: 0.2749214172363281/0.49013441801071167 G: 1.1354995965957642 (Real: [3.934155328273773, 1.2274537388027176], Fake: [4.577834060192108, 1.393208670929004]) \n",
            "24600: D: 0.5700423121452332/0.5252894163131714 G: 1.0013759136199951 (Real: [4.290030632019043, 1.1893635309347594], Fake: [4.445355552434921, 1.1365457814388604]) \n",
            "24800: D: 1.0919544696807861/1.028017282485962 G: 0.6483206152915955 (Real: [3.9626020121574403, 1.137480892815029], Fake: [3.534968320131302, 1.1903486856670746]) \n",
            "25000: D: 0.6093419194221497/0.7405123710632324 G: 0.6572816967964172 (Real: [4.033619622588158, 1.2332313241216917], Fake: [3.5937929439544676, 1.2142723689936152]) \n",
            "25200: D: 0.7726871371269226/0.5855985879898071 G: 0.7918116450309753 (Real: [4.090938534736633, 1.3949462412043578], Fake: [4.2230645632743835, 1.2558849685468236]) \n",
            "25400: D: 0.5612648725509644/0.7898027896881104 G: 0.5262370705604553 (Real: [3.8638631004095076, 1.2079320133999638], Fake: [3.995733152627945, 1.279864009177066]) \n",
            "25600: D: 0.6380051374435425/0.8562763333320618 G: 0.48965442180633545 (Real: [3.958236447572708, 1.285593133556949], Fake: [4.0331833148002625, 1.2053937334876361]) \n",
            "25800: D: 0.6192153692245483/0.6732902526855469 G: 0.6050824522972107 (Real: [4.238979849815369, 1.1483106630550977], Fake: [4.10097980260849, 1.2230097742176926]) \n",
            "26000: D: 0.6859062314033508/0.6246206164360046 G: 0.6307489275932312 (Real: [4.23225023150444, 1.1753374198167614], Fake: [4.293777759075165, 1.3518549551322934]) \n",
            "26200: D: 0.6897248029708862/0.8786784410476685 G: 0.6162384152412415 (Real: [3.8194650638103487, 1.1849667502214247], Fake: [4.061853575706482, 1.329985722986612]) \n",
            "26400: D: 0.7286075353622437/0.7019415497779846 G: 0.5349945425987244 (Real: [4.08377914071083, 1.1384888331522969], Fake: [3.809261109828949, 1.2447328706919352]) \n",
            "26600: D: 0.7134578227996826/0.6045677065849304 G: 0.6864928007125854 (Real: [3.844428322315216, 1.270794850571786], Fake: [3.9124097275733947, 1.2057145319133642]) \n",
            "26800: D: 0.7367126941680908/0.7675136923789978 G: 0.671442449092865 (Real: [4.100063326358796, 1.1930360399501454], Fake: [3.9293388438224794, 1.2139797656319729]) \n",
            "27000: D: 0.6930216550827026/0.6006242036819458 G: 0.8216729164123535 (Real: [3.9231527984142303, 1.1202988263724816], Fake: [4.215056272745133, 1.4077120955336528]) \n",
            "27200: D: 0.6448602676391602/0.7827561497688293 G: 0.5499628782272339 (Real: [4.135144684314728, 1.3283002476878532], Fake: [3.567266082763672, 1.1134986099414328]) \n",
            "27400: D: 0.736648440361023/0.6861415505409241 G: 0.6413449048995972 (Real: [4.176524376869201, 1.2593062838778417], Fake: [4.32246022939682, 1.0779706156923838]) \n",
            "27600: D: 0.935286819934845/0.6861449480056763 G: 0.7816941142082214 (Real: [4.078830805420876, 1.3505183837015753], Fake: [4.066063902378082, 1.2438154972580941]) \n",
            "27800: D: 0.7334421873092651/0.7386684417724609 G: 0.6734334230422974 (Real: [3.9960537612438203, 1.0994391273721786], Fake: [3.5425292932987213, 1.046521885793368]) \n",
            "28000: D: 0.6596799492835999/0.6768351793289185 G: 0.6785570383071899 (Real: [4.14459267616272, 1.303358111484011], Fake: [4.508256266117096, 1.2160637137848054]) \n",
            "28200: D: 0.6088378429412842/0.6660137176513672 G: 0.6809819936752319 (Real: [4.047795694470405, 1.0668208784617037], Fake: [3.954435962438583, 1.2672521112884434]) \n",
            "28400: D: 0.7040595412254333/0.7123610377311707 G: 0.6905146837234497 (Real: [4.082719621062279, 1.3732306308228222], Fake: [3.7793212473392486, 1.306210364948435]) \n",
            "28600: D: 0.7363271713256836/0.7274084687232971 G: 0.7427864074707031 (Real: [3.959789969921112, 1.2916916642131315], Fake: [4.578598577976226, 1.2533790025727638]) \n",
            "28800: D: 0.5025894641876221/0.8359308838844299 G: 0.5610354542732239 (Real: [4.058876385688782, 1.1706030133542165], Fake: [4.01799476146698, 1.3045159694985615]) \n",
            "29000: D: 0.7043195366859436/0.775009274482727 G: 0.6066510081291199 (Real: [3.928365161418915, 1.2579329200657314], Fake: [3.808704799413681, 1.3545489480186124]) \n",
            "29200: D: 0.6959170699119568/0.7967354655265808 G: 0.8178610801696777 (Real: [4.006979092955589, 1.3261120335647025], Fake: [4.373816356658936, 1.21851359726542]) \n",
            "29400: D: 0.6602917313575745/0.6436342597007751 G: 0.7262170910835266 (Real: [4.1643981206417084, 1.1197014758909318], Fake: [3.51104629278183, 1.147669277653423]) \n",
            "29600: D: 0.7133098840713501/0.6944707036018372 G: 0.7517645955085754 (Real: [3.9924234986305236, 1.2270295084118406], Fake: [4.1575507640838625, 1.0399653140252987]) \n",
            "29800: D: 0.6899421215057373/0.7259916663169861 G: 0.5690901279449463 (Real: [4.03614499092102, 1.308832609002849], Fake: [3.8569140315055845, 1.2137764356222192]) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jYHqAG5AwNGx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}