{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hello, Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabumjung/Tensorflow/blob/master/06-MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Lc6laX29IdYq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "cefcab93-2309-4f3b-fc22-c55260fd467a"
      },
      "cell_type": "code",
      "source": [
        "# 머신러닝 학습의 Hello World 와 같은 MNIST(손글씨 숫자 인식) 문제를 신경망으로 풀어봅니다.\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "# 텐서플로우에 기본 내장된 mnist 모듈을 이용하여 데이터를 로드합니다.\n",
        "# 지정한 폴더에 MNIST 데이터가 없는 경우 자동으로 데이터를 다운로드합니다.\n",
        "# one_hot 옵션은 레이블을 동물 분류 예제에서 보았던 one_hot 방식의 데이터로 만들어줍니다.\n",
        "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
        "\n",
        "#########\n",
        "# 신경망 모델 구성\n",
        "######\n",
        "# 입력 값의 차원은 [배치크기, 특성값] 으로 되어 있습니다.\n",
        "# 손글씨 이미지는 28x28 픽셀로 이루어져 있고, 이를 784개의 특성값으로 정합니다.\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 결과는 0~9 의 10 가지 분류를 가집니다.\n",
        "Y = tf.placeholder(tf.float32, [None, 10])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-76fedc77f5b2>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XTyik5Y_Ly40",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1969
        },
        "outputId": "ec196e0d-2a1b-4e9f-e5ff-22254e2e9fad"
      },
      "cell_type": "code",
      "source": [
        "#label이 onehot encoding잘되었는지 확인\n",
        "#2번째 label은 4임 따라서 0, 0, 0, 0, 1으로 표현되어야 함\n",
        "print(mnist.train.labels[1])\n",
        "#image데이터 확인(flat형태로 잘되었는지 확인)\n",
        "print(mnist.train.images[1]) "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "[0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.5019608 1.        1.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.2509804\n",
            " 1.        1.        1.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.2509804 1.        0.7490196\n",
            " 0.        0.        0.        0.        0.        0.        0.7490196\n",
            " 1.        1.        1.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        1.        1.        1.\n",
            " 0.2509804 0.        0.        0.        0.        0.5019608 1.\n",
            " 1.        1.        0.2509804 0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.5019608 1.        1.        1.\n",
            " 0.        0.        0.        0.        0.2509804 1.        1.\n",
            " 1.        0.5019608 0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        1.        1.        1.        0.5019608\n",
            " 0.        0.        0.        0.        1.        1.        1.\n",
            " 1.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.7490196 1.        1.        1.        0.5019608\n",
            " 0.        0.        0.        0.7490196 1.        1.        1.\n",
            " 0.2509804 0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.2509804 1.        1.        1.        1.        0.\n",
            " 0.        0.        0.5019608 1.        1.        1.        0.7490196\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.7490196 1.        1.        1.        0.2509804 0.\n",
            " 0.        0.2509804 1.        1.        1.        0.7490196 0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.5019608 1.        1.        1.        0.7490196 0.        0.\n",
            " 0.        1.        1.        1.        1.        0.2509804 0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.5019608 1.        1.        1.        1.        1.        1.\n",
            " 1.        1.        1.        1.        0.7490196 0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.5019608 1.        1.        1.        1.        1.        1.\n",
            " 1.        1.        1.        0.7490196 0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.5019608 1.        1.        1.        1.        1.        1.\n",
            " 1.        1.        0.7490196 0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.2509804 0.5019608 0.5019608 1.        1.        1.        1.\n",
            " 1.        1.        0.5019608 0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.2509804 1.        1.\n",
            " 1.        0.7490196 0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.7490196 1.        1.\n",
            " 1.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.5019608 1.        1.        1.\n",
            " 0.2509804 0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.7490196 1.        1.        0.5019608\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.7490196 1.        1.        1.        0.2509804\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.5019608 1.        1.        0.7490196 0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.       ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y2KdYslGKRZq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "30894e9f-78dd-4581-8a67-feb2fe9f5445"
      },
      "cell_type": "code",
      "source": [
        "# MNIST의 훈련용 데이터셋을 그림으로 표시하기\n",
        "import numpy as np\n",
        "\n",
        "print(mnist.train.images.shape[1])\n",
        "arr = np.array(mnist.train.images[1])\n",
        "arr.shape = (28, 28)\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(arr)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f82f7a49048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8301fbe240>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "srfj_pFnIqys",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 신경망의 레이어는 다음처럼 구성합니다.\n",
        "# 784(입력 특성값)\n",
        "#   -> 256 (히든레이어 뉴런 갯수) -> 256 (히든레이어 뉴런 갯수)\n",
        "#   -> 10 (결과값 0~9 분류)\n",
        "W1 = tf.Variable(tf.random_normal([784, 256], stddev=0.01))\n",
        "# 입력값에 가중치를 곱하고 ReLU 함수를 이용하여 레이어를 만듭니다.\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1))\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal([256, 256], stddev=0.01))\n",
        "# L1 레이어의 출력값에 가중치를 곱하고 ReLU 함수를 이용하여 레이어를 만듭니다.\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2))\n",
        "\n",
        "W3 = tf.Variable(tf.random_normal([256, 10], stddev=0.01))\n",
        "# 최종 모델의 출력값은 W3 변수를 곱해 10개의 분류를 가지게 됩니다.\n",
        "model = tf.matmul(L2, W3)\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ksiWcNhKIrfK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "outputId": "80d9f546-de5d-4e3f-a8d4-a0526c7ec53d"
      },
      "cell_type": "code",
      "source": [
        "#########\n",
        "# 신경망 모델 학습\n",
        "######\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "batch_size = 100\n",
        "total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "for epoch in range(30):\n",
        "    total_cost = 0\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        # 텐서플로우의 mnist 모델의 next_batch 함수를 이용해\n",
        "        # 지정한 크기인 배치크기 만큼 학습할 데이터를 가져옵니다.\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "        _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "        total_cost += cost_val\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1),\n",
        "          'Avg. cost =', '{:.3f}'.format(total_cost / total_batch))\n",
        "\n",
        "print('최적화 완료!')\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 Avg. cost = 0.400\n",
            "Epoch: 0002 Avg. cost = 0.146\n",
            "Epoch: 0003 Avg. cost = 0.094\n",
            "Epoch: 0004 Avg. cost = 0.070\n",
            "Epoch: 0005 Avg. cost = 0.051\n",
            "Epoch: 0006 Avg. cost = 0.040\n",
            "Epoch: 0007 Avg. cost = 0.032\n",
            "Epoch: 0008 Avg. cost = 0.024\n",
            "Epoch: 0009 Avg. cost = 0.022\n",
            "Epoch: 0010 Avg. cost = 0.019\n",
            "Epoch: 0011 Avg. cost = 0.018\n",
            "Epoch: 0012 Avg. cost = 0.012\n",
            "Epoch: 0013 Avg. cost = 0.013\n",
            "Epoch: 0014 Avg. cost = 0.012\n",
            "Epoch: 0015 Avg. cost = 0.011\n",
            "Epoch: 0016 Avg. cost = 0.009\n",
            "Epoch: 0017 Avg. cost = 0.011\n",
            "Epoch: 0018 Avg. cost = 0.011\n",
            "Epoch: 0019 Avg. cost = 0.008\n",
            "Epoch: 0020 Avg. cost = 0.010\n",
            "Epoch: 0021 Avg. cost = 0.008\n",
            "Epoch: 0022 Avg. cost = 0.007\n",
            "Epoch: 0023 Avg. cost = 0.009\n",
            "Epoch: 0024 Avg. cost = 0.005\n",
            "Epoch: 0025 Avg. cost = 0.006\n",
            "Epoch: 0026 Avg. cost = 0.011\n",
            "Epoch: 0027 Avg. cost = 0.006\n",
            "Epoch: 0028 Avg. cost = 0.005\n",
            "Epoch: 0029 Avg. cost = 0.008\n",
            "Epoch: 0030 Avg. cost = 0.004\n",
            "최적화 완료!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DzuEKdnAIuuG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a9e80f30-380e-45f3-c005-725a6c05c365"
      },
      "cell_type": "code",
      "source": [
        "#########\n",
        "# 결과 확인\n",
        "######\n",
        "# model 로 예측한 값과 실제 레이블인 Y의 값을 비교합니다.\n",
        "# tf.argmax 함수를 이용해 예측한 값에서 가장 큰 값을 예측한 레이블이라고 평가합니다.\n",
        "# 예) [0.1 0 0 0.7 0 0.2 0 0 0 0] -> 3\n",
        "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "print('정확도:', sess.run(accuracy,\n",
        "                        feed_dict={X: mnist.test.images,\n",
        "                                   Y: mnist.test.labels}))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정확도: 0.9815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VUyHLjJHJCm2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}