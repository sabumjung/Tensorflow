{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hello, Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/sabumjung/Tensorflow/blob/master/JA_01_Simple%20Policy.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "VtiXiAACZ4wi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LQax6GYgZ6iR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#List out our bandit arms. \n",
        "#Currently arm 4 (index #3) is set to most often provide a positive reward.\n",
        "#해당 밴딧암의 값보다 크면 양의 reward를 받고, 작으면 음의 reward를 받는다.\n",
        "bandit_arms = [0.2,-5,-0.2,-2]\n",
        "num_arms = len(bandit_arms)\n",
        "def pullBandit(bandit):\n",
        "    #Get a random number.\n",
        "    result = np.random.randn(1)\n",
        "    if result > bandit:\n",
        "        #return a positive reward.\n",
        "        return 1\n",
        "    else:\n",
        "        #return a negative reward.\n",
        "        return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rRCmC9NjZ8Ex",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#graph를 reset하고 정의한다.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "#These two lines established the feed-forward part of the network. \n",
        "#가중치 행렬 : 초기 1로 설정함 [1,1,1,1]\n",
        "weights = tf.Variable(tf.ones([num_arms]))\n",
        "#출력 : 가중치연산결과에 대해 softmax함수를 적용함\n",
        "#graph를 완성하여 output에 넣는다.\n",
        "#weights에 대한 softmax활성화함수를 계산하는 연산을 정의하고 graph인 output에 저장함\n",
        "output = tf.nn.softmax(weights)\n",
        "\n",
        "#The next six lines establish the training proceedure. \n",
        "#We feed the reward and chosen action into the network to compute the loss, and use it to update the network.\n",
        "#모델링 훈련 파라미터(reward, action값을 저장하기 위한 플레이스홀더 정의)\n",
        "reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
        "action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
        "\n",
        "responsible_output = tf.slice(output,action_holder,[1])\n",
        "loss = -(tf.log(responsible_output)*reward_holder)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
        "update = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TsdET83LZ-b9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "206e5722-5423-4ac5-88c4-434123e7e37d"
      },
      "cell_type": "code",
      "source": [
        "total_episodes = 1000 #Set total number of episodes to train agent on.\n",
        "total_reward = np.zeros(num_arms) #Set scoreboard for bandit arms to 0.\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the tensorflow graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    i = 0\n",
        "    while i < total_episodes:\n",
        "        \n",
        "        #Choose action according to Boltzmann distribution.\n",
        "        actions = sess.run(output)\n",
        "       \n",
        "        #actions구성요소에서 랜덤선택수행, 각 구성요소 선택확률은 p값임  \n",
        "        a = np.random.choice(actions, p=actions)\n",
        "        #조건문 actions == a의 결과에서 가장 큰값인 1(TRUE)을 갖는 열인덱스를 찾아내서 action에 저장함\n",
        "        action = np.argmax(actions == a)\n",
        "        # print(actions, actions==a, action)\n",
        "\n",
        "        #몇번째 밴딧암을 선택할지 pullBandit의 인수로 전달함\n",
        "        reward = pullBandit(bandit_arms[action]) #Get our reward from picking one of the bandit arms.\n",
        "        \n",
        "        #신경망을 업데이트함\n",
        "        #reward, action이력을 저장함\n",
        "        _,resp,ww = sess.run([update,responsible_output,weights], feed_dict={reward_holder:[reward],action_holder:[action]})\n",
        "        \n",
        "        #score이력을 저장함\n",
        "        total_reward[action] += reward\n",
        "        if i % 50 == 0:\n",
        "            print(\"Running reward for the \" + str(num_arms) + \" arms of the bandit: \" + str(total_reward))\n",
        "        i+=1\n",
        "print(\"\\nThe agent thinks arm \" + str(np.argmax(ww)+1) + \" is the most promising....\")\n",
        "if np.argmax(ww) == np.argmax(-np.array(bandit_arms)):\n",
        "    print(\"...and it was right!\")\n",
        "else:\n",
        "    print(\"...and it was wrong!\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running reward for the 4 arms of the bandit: [1. 0. 0. 0.]\n",
            "Running reward for the 4 arms of the bandit: [ 1. 10.  1. 15.]\n",
            "Running reward for the 4 arms of the bandit: [ 6. 29.  4. 20.]\n",
            "Running reward for the 4 arms of the bandit: [ 6. 41.  9. 31.]\n",
            "Running reward for the 4 arms of the bandit: [ 3. 53.  5. 44.]\n",
            "Running reward for the 4 arms of the bandit: [-5. 68.  4. 52.]\n",
            "Running reward for the 4 arms of the bandit: [-1. 85. 10. 67.]\n",
            "Running reward for the 4 arms of the bandit: [ 0. 96. 13. 80.]\n",
            "Running reward for the 4 arms of the bandit: [ -4. 111.  20.  88.]\n",
            "Running reward for the 4 arms of the bandit: [ -8. 126.  26. 101.]\n",
            "Running reward for the 4 arms of the bandit: [-12. 139.  29. 117.]\n",
            "Running reward for the 4 arms of the bandit: [-15. 155.  32. 133.]\n",
            "Running reward for the 4 arms of the bandit: [-17. 170.  31. 147.]\n",
            "Running reward for the 4 arms of the bandit: [-21. 192.  32. 156.]\n",
            "Running reward for the 4 arms of the bandit: [-27. 202.  35. 171.]\n",
            "Running reward for the 4 arms of the bandit: [-30. 217.  35. 187.]\n",
            "Running reward for the 4 arms of the bandit: [-32. 229.  34. 204.]\n",
            "Running reward for the 4 arms of the bandit: [-27. 249.  33. 216.]\n",
            "Running reward for the 4 arms of the bandit: [-25. 267.  38. 227.]\n",
            "Running reward for the 4 arms of the bandit: [-25. 282.  39. 243.]\n",
            "\n",
            "The agent thinks arm 2 is the most promising....\n",
            "...and it was right!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KloVHREEaDI0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}